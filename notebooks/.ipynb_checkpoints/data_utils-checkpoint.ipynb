{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Utility functions for handling dataset and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_file(filepath, word_dict):\n",
    "    with open(filepath) as ifile:\n",
    "        return [word_dict.get(w, 0) for w in ifile.read().split(' ')]\n",
    "\n",
    "\n",
    "def discover_dataset(path, wdict):\n",
    "    dataset = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for sfile in [f for f in files if '.txt' in f]:\n",
    "            filepath = os.path.join(root, sfile)\n",
    "            dataset.append(convert_file(filepath, wdict))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_dataset(dataset, maxlen):\n",
    "    return np.array(\n",
    "        [np.pad(r, (0, maxlen-len(r)), mode='constant') if len(r) < maxlen else np.array(r[:maxlen])\n",
    "         for r in dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for dataset related operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IMDBDataset():\n",
    "    def __init__(self, path, dict_path, maxlen=128):\n",
    "        pos_path = os.path.join(path, 'pos')\n",
    "        neg_path = os.path.join(path, 'neg')\n",
    "\n",
    "        with open(dict_path, 'rb') as dfile:\n",
    "            wdict = pickle.load(dfile)\n",
    "\n",
    "        self.pos_dataset = pad_dataset(discover_dataset(pos_path, wdict), maxlen).astype('i')\n",
    "        self.neg_dataset = pad_dataset(discover_dataset(neg_path, wdict), maxlen).astype('i')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_dataset) + len(self.neg_dataset)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        is_neg = i >= len(self.pos_dataset)\n",
    "        dataset = self.neg_dataset if is_neg else self.pos_dataset\n",
    "        idx = i - len(self.pos_dataset) if is_neg else i\n",
    "        label = [0, 1] if is_neg else [1, 0]\n",
    "        \n",
    "        print (type(dataset[idx]))\n",
    "        return (dataset[idx], np.array(label, dtype=np.int32))\n",
    "    \n",
    "    def load(self):\n",
    "        \n",
    "        dataset = np.concatenate((self.pos_dataset, self.neg_dataset))\n",
    "        labels = []\n",
    "        \n",
    "        for idx in range (0, len(self.pos_dataset)):\n",
    "            labels.append([1, 0])\n",
    "        \n",
    "        for idx in range (0, len(self.neg_dataset)):\n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        return dataset, np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example usage\n",
    "dataset = IMDBDataset('./data/aclImdb/train', './data/vocab.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = dataset.load()\n",
    "\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "\n",
    "X_train = X[shuffle_indices]\n",
    "Y_train = Y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Function for handling word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_embeddings(path, size, dimensions):\n",
    "    # premature memory optimization :)\n",
    "    ret = np.zeros((size, dimensions), dtype=np.float32)\n",
    "\n",
    "    # As embedding matrix could be quite big we 'stream' it into output file\n",
    "    # chunk by chunk. One chunk shape could be [size // 10, dimensions].\n",
    "    # So to load whole matrix we read the file until it's exhausted.\n",
    "    size = os.stat(path).st_size\n",
    "    with open(path, 'rb') as ifile:\n",
    "        pos = 0\n",
    "        idx = 0\n",
    "        while pos < size:\n",
    "            chunk = np.load(ifile)\n",
    "            chunk_size = chunk.shape[0]\n",
    "            ret[idx:idx+chunk_size, :] = chunk\n",
    "            idx += chunk_size\n",
    "            pos = ifile.tell()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example usage\n",
    "embedding_path = './data/embeddings.npy'\n",
    "\n",
    "em = load_embeddings(embedding_path, 75099, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [WIP] function for creating batches\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    print (\"Generating batch iterator ...\")\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
