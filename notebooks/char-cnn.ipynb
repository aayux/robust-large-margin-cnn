{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pckl\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "\n",
    "class YelpDataset():    \n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "   \n",
    "    def load(self):\n",
    "        \n",
    "        x, y = self.generate_data()\n",
    "        \n",
    "        print(\"X: {}\".format(x.shape))\n",
    "        print(\"Y: {}\".format(y.shape))\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def generate_data(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        with open(self.path) as dfile:\n",
    "            count = 0\n",
    "            \n",
    "            for line in dfile:\n",
    "                review = json.loads(line)\n",
    "                stars = review[\"stars\"]\n",
    "                text = review[\"text\"]\n",
    "                \n",
    "                # Non neutral reviews\n",
    "                if stars != 3:\n",
    "                    clipped = self.clip_seq(list(text.lower()))\n",
    "                    padded = self.pad_seq(clipped)\n",
    "                    int_seq = self.str_to_int8(padded)\n",
    "                    if stars == 1 or stars == 2:\n",
    "                        x.append(int_seq)\n",
    "                        y.append([1, 0])\n",
    "                    elif stars == 4 or stars == 5:\n",
    "                        x.append(int_seq)\n",
    "                        y.append([0, 1])\n",
    "                    count += 1\n",
    "                    if count % 1000 == 0:\n",
    "                        print(\"{} non-neutral instances processed\".format(count))\n",
    "                        break\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "    def clip_seq(self, char_seq):\n",
    "        if len(char_seq) > 1014:\n",
    "            char_seq = char_seq[-1014:]\n",
    "        return char_seq\n",
    "\n",
    "\n",
    "    def pad_seq(self, char_seq, seq_length=1014, pad_char=\" \"):\n",
    "        pad_width = seq_length - len(char_seq)\n",
    "        padded_seq = char_seq + [pad_char] * pad_width\n",
    "        return padded_seq\n",
    "\n",
    "\n",
    "    def str_to_int8(self, char_seq):\n",
    "        return np.array([alphabet.find(char) for char in char_seq], dtype=np.int8)\n",
    "\n",
    "\n",
    "def one_hot_x(x, y, start_idx, end_idx):\n",
    "    x_batch = x[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    one_hot_batch = []\n",
    "    \n",
    "    binarizer = LabelBinarizer()\n",
    "    binarizer.fit(range(len(alphabet)))\n",
    "    \n",
    "    for x in x_batch:\n",
    "        one_hot_batch.append(binarizer.transform(x))\n",
    "    one_hot_batch = np.array([one_hot_batch])\n",
    "    x_batch = np.transpose(one_hot_batch, (1, 3, 2, 0))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def batch_iter(x, y, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    print (\"Generating batch iterator ...\")\n",
    "    data_size = len(x)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            x_shuff = x[shuffle_indices]\n",
    "            y_shuff = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuff = x\n",
    "            y_shuff = y\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, data_size)\n",
    "            x_batch, y_batch = one_hot_x(x_shuff, y_shuff, start_idx, end_idx)\n",
    "            yield list(zip(x_batch, y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    based on the Character-level Convolutional Networks for Text Classification paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, quantization_size, num_classes, filter_sizes, num_filters, \n",
    "        learning_rate, l2_reg_lambda=0.0, jac_reg=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, quantization_size, sequence_length, 1], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # ================ Layer 1 ================\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            filter_shape = [quantization_size, filter_sizes[0], 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(self.input_x, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv1\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, 1, 3, 1],\n",
    "                strides=[1, 1, 3, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool1\")\n",
    "\n",
    "        # ================ Layer 2 ================\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            filter_shape = [1, filter_sizes[1], num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv2\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, 1, 3, 1],\n",
    "                strides=[1, 1, 3, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool2\")\n",
    "\n",
    "        # ================ Layer 3 ================\n",
    "        with tf.name_scope(\"conv-3\"):\n",
    "            filter_shape = [1, filter_sizes[2], num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv3\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 4 ================\n",
    "        with tf.name_scope(\"conv-4\"):\n",
    "            filter_shape = [1, filter_sizes[3], num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv4\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 5 ================\n",
    "        with tf.name_scope(\"conv-5\"):\n",
    "            filter_shape = [1, filter_sizes[4], num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv5\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 6 ================\n",
    "        with tf.name_scope(\"conv-maxpool-6\"):\n",
    "            filter_shape = [1, filter_sizes[5], num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv6\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, 1, 3, 1],\n",
    "                strides=[1, 1, 3, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool6\")\n",
    "\n",
    "        # ================ Layer 7 ================\n",
    "        feature_vec_length = 34 * num_filters\n",
    "        h_pool_flat = tf.reshape(pooled, [-1, feature_vec_length])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop1 = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([feature_vec_length, 1024], stddev=0.05), name=\"W\")\n",
    "            # W = tf.get_variable(\"W\", shape=[num_features_total, 1024],\n",
    "            #                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(drop1, W, b), name=\"fc-1-out\")\n",
    "\n",
    "        # ================ Layer 8 ================\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop2 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, 1024], stddev=0.05), name=\"W\")\n",
    "            # W = tf.get_variable(\"W\", shape=[1024, 1024],\n",
    "            #                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop2, W, b), name=\"fc-2-out\")\n",
    "\n",
    "        # ================ Layer 9 ================\n",
    "        # Fully connected layer 3\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            scores = tf.nn.xw_plus_b(fc_2_output, W, b, name=\"output\")\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "        # ================ Loss and Accuracy ================\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "No data dump found. Pickling dataset ...\n",
      "1000 non-neutral instances processed\n",
      "X: (1000, 1014)\n",
      "Y: (1000, 2)\n",
      "Dataset loaded. Preparing data ...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print (\"Loading Dataset ...\")\n",
    "\n",
    "pcklfile = \"./data/dump.pckl\"\n",
    "\n",
    "if not os.path.isfile(pcklfile):\n",
    "    print (\"No data dump found. Pickling dataset ...\")\n",
    "    dataset = YelpDataset('./data/review.json')\n",
    "    X, Y = dataset.load()\n",
    "    pckl.dump([X, Y], open(pcklfile, \"wb\"))\n",
    "else:\n",
    "    X, Y = pckl.load(open(pcklfile, \"rb\"))\n",
    "\n",
    "print (\"Dataset loaded. Preparing data ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val split: 800/200\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "\n",
    "x_shuff = X[shuffle_indices]\n",
    "y_shuff = Y[shuffle_indices]\n",
    "\n",
    "# Percentage of the training data to use for validation\n",
    "val_sample = .2\n",
    "\n",
    "# Split train/test set\n",
    "idx = -1 * int(val_sample * float(len(Y)))\n",
    "x_train, x_val = x_shuff[:idx], x_shuff[idx:]\n",
    "y_train, y_val = y_shuff[:idx], y_shuff[idx:]\n",
    "print(\"Train/Val split: {:d}/{:d}\".format(len(y_train), len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 1014\n",
    "quantization_size = 70\n",
    "num_classes = 2\n",
    "\n",
    "# Model parameters\n",
    "filter_sizes = (7, 7, 3, 3, 3, 3)\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0.0\n",
    "jac_reg = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "checkpoint_every = 1000\n",
    "validate_every = 5000\n",
    "num_checkpoints = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Dimensions:\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/W:0/grad/hist is illegal; using conv-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/W:0/grad/hist is illegal; using conv-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/W:0/grad/sparsity is illegal; using conv-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/W:0/grad/sparsity is illegal; using conv-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/b:0/grad/hist is illegal; using conv-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/b:0/grad/hist is illegal; using conv-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/b:0/grad/sparsity is illegal; using conv-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-3/b:0/grad/sparsity is illegal; using conv-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/W:0/grad/hist is illegal; using conv-4/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/W:0/grad/hist is illegal; using conv-4/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/W:0/grad/sparsity is illegal; using conv-4/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/W:0/grad/sparsity is illegal; using conv-4/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/b:0/grad/hist is illegal; using conv-4/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/b:0/grad/hist is illegal; using conv-4/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/b:0/grad/sparsity is illegal; using conv-4/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-4/b:0/grad/sparsity is illegal; using conv-4/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/W:0/grad/hist is illegal; using conv-5/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/W:0/grad/hist is illegal; using conv-5/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/W:0/grad/sparsity is illegal; using conv-5/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/W:0/grad/sparsity is illegal; using conv-5/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/b:0/grad/hist is illegal; using conv-5/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/b:0/grad/hist is illegal; using conv-5/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/b:0/grad/sparsity is illegal; using conv-5/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-5/b:0/grad/sparsity is illegal; using conv-5/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/aayush/char-level-cnn/runs/1512759807\n",
      "\n",
      "Generating batch iterator ...\n",
      "2017-12-09T00:33:39.312370: step 1, loss 1.81456, acc 0.40625\n",
      "2017-12-09T00:33:49.690599: step 2, loss 20.8427, acc 0.804688\n",
      "2017-12-09T00:33:59.519883: step 3, loss 6.29215, acc 0.789062\n",
      "2017-12-09T00:34:10.259297: step 4, loss 1.34167, acc 0.804688\n",
      "2017-12-09T00:34:21.118806: step 5, loss 0.90234, acc 0.382812\n",
      "2017-12-09T00:34:32.246150: step 6, loss 0.547548, acc 0.796875\n",
      "2017-12-09T00:34:35.967202: step 7, loss 0.812425, acc 0.75\n",
      "2017-12-09T00:34:50.184814: step 8, loss 0.467912, acc 0.8125\n",
      "2017-12-09T00:35:04.641061: step 9, loss 0.606213, acc 0.75\n",
      "2017-12-09T00:35:17.318532: step 10, loss 0.545974, acc 0.804688\n",
      "2017-12-09T00:35:30.033896: step 11, loss 0.614976, acc 0.789062\n",
      "2017-12-09T00:35:43.308117: step 12, loss 0.54714, acc 0.796875\n",
      "2017-12-09T00:35:54.887713: step 13, loss 0.6652, acc 0.773438\n",
      "2017-12-09T00:35:58.696428: step 14, loss 0.554933, acc 0.8125\n",
      "2017-12-09T00:36:12.206746: step 15, loss 0.518999, acc 0.796875\n",
      "2017-12-09T00:36:27.274920: step 16, loss 0.655533, acc 0.757812\n",
      "2017-12-09T00:36:40.318551: step 17, loss 0.502948, acc 0.804688\n",
      "2017-12-09T00:36:54.968618: step 18, loss 0.526742, acc 0.804688\n",
      "2017-12-09T00:37:10.313480: step 19, loss 0.507842, acc 0.828125\n",
      "2017-12-09T00:37:23.026829: step 20, loss 0.582239, acc 0.789062\n",
      "2017-12-09T00:37:26.722910: step 21, loss 0.604048, acc 0.75\n",
      "2017-12-09T00:37:40.010797: step 22, loss 0.500321, acc 0.804688\n",
      "2017-12-09T00:37:52.617641: step 23, loss 0.59135, acc 0.757812\n",
      "2017-12-09T00:38:05.713933: step 24, loss 0.494766, acc 0.804688\n",
      "2017-12-09T00:38:19.090479: step 25, loss 0.543729, acc 0.773438\n",
      "2017-12-09T00:38:32.480840: step 26, loss 0.457696, acc 0.804688\n",
      "2017-12-09T00:38:50.804289: step 27, loss 0.477456, acc 0.820312\n",
      "2017-12-09T00:38:55.778471: step 28, loss 0.363384, acc 0.84375\n",
      "2017-12-09T00:39:10.327789: step 29, loss 0.403337, acc 0.867188\n",
      "2017-12-09T00:39:23.684298: step 30, loss 0.651934, acc 0.78125\n",
      "2017-12-09T00:39:38.700936: step 31, loss 0.637297, acc 0.703125\n",
      "2017-12-09T00:39:51.924606: step 32, loss 0.732299, acc 0.515625\n",
      "2017-12-09T00:40:06.935441: step 33, loss 0.554078, acc 0.75\n",
      "2017-12-09T00:40:20.578600: step 34, loss 0.64634, acc 0.765625\n",
      "2017-12-09T00:40:23.925408: step 35, loss 0.563703, acc 0.8125\n",
      "2017-12-09T00:40:38.980425: step 36, loss 0.45083, acc 0.851562\n",
      "2017-12-09T00:40:51.510470: step 37, loss 0.610719, acc 0.773438\n",
      "2017-12-09T00:41:05.965664: step 38, loss 0.564669, acc 0.773438\n",
      "2017-12-09T00:41:19.142664: step 39, loss 0.453126, acc 0.851562\n",
      "2017-12-09T00:41:34.975363: step 40, loss 0.574729, acc 0.757812\n",
      "2017-12-09T00:41:48.457952: step 41, loss 0.524023, acc 0.8125\n",
      "2017-12-09T00:41:51.998495: step 42, loss 0.506364, acc 0.8125\n",
      "2017-12-09T00:42:05.876866: step 43, loss 0.546901, acc 0.773438\n",
      "2017-12-09T00:42:17.886093: step 44, loss 0.49857, acc 0.804688\n",
      "2017-12-09T00:42:33.701517: step 45, loss 0.512459, acc 0.796875\n",
      "2017-12-09T00:42:49.736008: step 46, loss 0.558499, acc 0.78125\n",
      "2017-12-09T00:43:08.894658: step 47, loss 0.443823, acc 0.835938\n",
      "2017-12-09T00:43:27.705921: step 48, loss 0.459458, acc 0.828125\n",
      "2017-12-09T00:43:31.556094: step 49, loss 0.487689, acc 0.8125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-9aa9e983608a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-9aa9e983608a>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     73\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m     74\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training ...\")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN(\n",
    "            sequence_length=sequence_length,\n",
    "            quantization_size=quantization_size,\n",
    "            num_classes=num_classes,\n",
    "            filter_sizes=filter_sizes,\n",
    "            num_filters=num_filters,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            jac_reg=jac_reg)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)             \n",
    "                \n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "      \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Val summaries\n",
    "        val_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        val_summary_dir = os.path.join(out_dir, \"summaries\", \"val\")\n",
    "        val_summary_writer = tf.summary.FileWriter(val_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "               \n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def validation_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, val_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = batch_iter(x_train, y_train, batch_size, num_epochs)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % validate_every == 0:\n",
    "                print(\"\\nValidation: \")\n",
    "                validation_step(x_val, y_val, writer=val_summary_writer)\n",
    "\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
