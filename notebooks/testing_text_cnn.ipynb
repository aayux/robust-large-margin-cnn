{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_utils as utils\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "#from text_cnn import TextCNN\n",
    "from data_utils import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Dataset loaded. Preparing data and loading embeddings ...\n",
      "Embeddings loaded. Initialising model hyperparameters ...\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 128\n",
    "num_classes = 2\n",
    "vocab_size = 75099\n",
    "embedding_dim = 300\n",
    "\n",
    "print (\"Loading Dataset ...\")\n",
    "dataset = IMDBDataset('/home/aayush/robust-large-margin-cnn-develop/data/aclImdb/train', '/home/aayush/robust-large-margin-cnn-develop/data/vocab.pckl')\n",
    "X, Y = dataset.load()\n",
    "print (\"Dataset loaded. Preparing data and loading embeddings ...\")\n",
    "\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "\n",
    "X_train = X[shuffle_indices]\n",
    "Y_train = Y[shuffle_indices]\n",
    "\n",
    "embedding_path = '/home/aayush/robust-large-margin-cnn-develop/data/embeddings.npy'\n",
    "embedding = utils.load_embeddings(embedding_path, vocab_size, embedding_dim)\n",
    "print (\"Embeddings loaded. Initialising model hyperparameters ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75099, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(self, shape):\n",
    "    return tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "\n",
    "def init_bias(self, shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "\n",
    "def convolution(self, inp, kernelShape, biasShape):\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"weight\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters // 2]), name=\"bias\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        self.h_pool,\n",
    "        W,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"convolution\")\n",
    "    return conv\n",
    "\n",
    "def non_linearity():\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "def maxpool(self, inp, kernelShape):\n",
    "        pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length // 2 - filter_size, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                print('Maxpool1-{}: {}'.format(filter_size, pooled.get_shape()))\n",
    "                pooled_outputs.append(pooled)\n",
    "        return pooled\n",
    "\n",
    "def fully_connected(self, inp, inpShape, outShape, activation=False):\n",
    "    weights = self.init_weight([inpShape, outShape])\n",
    "    bias = self.init_bias(outShape)\n",
    "    out = tf.matmul(inp, weights) + bias\n",
    "    if activation:\n",
    "        return tf.nn.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Similar to VGG-Net architecture but with embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "       \n",
    "        # Embedding layer\n",
    "        self.word_embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "        self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "        self.embedding_init = self.word_embedding.assign(self.embedding_placeholder)\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.word_embedding, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        print('Embedding: {}'.format(self.embedded_chars_expanded.get_shape()))\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv1-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                print('Conv1-{}: {}'.format(filter_size, conv.get_shape()))\n",
    "                \n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length // 2 - filter_size, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                print('Maxpool1-{}: {}'.format(filter_size, pooled.get_shape()))\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        print('Concatenated: {}'.format(self.h_pool.get_shape()))\n",
    "        #self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "       \n",
    "        \n",
    "        with tf.name_scope(\"conv2-maxpool\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [4, 1, self.h_pool.get_shape()[3].value, num_filters // 2]\n",
    "            W_2 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_2\")\n",
    "            b_2 = tf.Variable(tf.constant(0.1, shape=[num_filters // 2]), name=\"b_2\")\n",
    "            conv_2 = tf.nn.conv2d(\n",
    "                self.h_pool,\n",
    "                W_2,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv_2\")\n",
    "            print('Conv2: {}'.format(conv_2.get_shape()))\n",
    "            \n",
    "            # Apply nonlinearity\n",
    "            h_2 = tf.nn.relu(tf.nn.bias_add(conv_2, b_2), name=\"relu_2\")            \n",
    "            \n",
    "            # Maxpooling over the outputs\n",
    "            self.pooled_2 = tf.nn.max_pool(\n",
    "                h_2,\n",
    "                ksize=[1, 128 // 8 - 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool_2\")\n",
    "            print('Maxpool2: {}'.format(self.pooled_2.get_shape()))\n",
    "            \n",
    "        with tf.name_scope(\"conv3-maxpool\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [6, 1, self.pooled_2.get_shape()[3].value, num_filters // 2]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_3\")\n",
    "            b_3 = tf.Variable(tf.constant(0.1, shape=[num_filters // 2]), name=\"b_3\")\n",
    "            conv_3 = tf.nn.conv2d(\n",
    "                self.pooled_2,\n",
    "                W_3,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv_3\")\n",
    "            print('Conv3: {}'.format(conv_3.get_shape()))\n",
    "            \n",
    "            # Apply nonlinearity\n",
    "            h_3 = tf.nn.relu(tf.nn.bias_add(conv_3, b_3), name=\"relu_3\")            \n",
    "            \n",
    "            # Maxpooling over the outputs\n",
    "            self.pooled_3 = tf.nn.max_pool(\n",
    "                h_3,\n",
    "                ksize=[1, conv_3.get_shape()[1].value, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool_3\")\n",
    "            print('Maxpool3: {}'.format(self.pooled_3.get_shape()))\n",
    "        \n",
    "        # Flatten into a long feature vector\n",
    "        self.h_pool_flat = tf.reshape(self.pooled_3, [-1, num_filters // 2])\n",
    "        print(self.h_pool_flat.get_shape())\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters // 2, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##To do\n",
    "- Rewrite each layer as a function of class\n",
    "- Introduce network depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "filter_sizes = [4, 5]\n",
    "num_filters = 64\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 50\n",
    "num_epochs = 10\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Embedding: (?, 128, 300, 1)\n",
      "Conv1-4: (?, 125, 1, 64)\n",
      "Maxpool1-4: (?, 66, 1, 64)\n",
      "Conv1-5: (?, 124, 1, 64)\n",
      "Maxpool1-5: (?, 66, 1, 64)\n",
      "Concatenated: (?, 66, 1, 128)\n",
      "Conv2: (?, 63, 1, 32)\n",
      "Maxpool2: (?, 49, 1, 32)\n",
      "Conv3: (?, 44, 1, 32)\n",
      "Maxpool3: (?, 1, 1, 32)\n",
      "(?, 32)\n",
      "Checkpoints and logs will be written into /home/aayush/robust-large-margin-cnn-develop/runs/1505134384\n",
      "\n",
      "Generating batch iterator ...\n",
      "2017-09-11T18:23:08.992502: step 1, loss 3.3024, acc 0.5\n",
      "2017-09-11T18:23:09.371872: step 2, loss 2.63103, acc 0.58\n",
      "2017-09-11T18:23:09.744287: step 3, loss 2.82357, acc 0.56\n",
      "2017-09-11T18:23:10.118422: step 4, loss 1.64471, acc 0.48\n",
      "2017-09-11T18:23:10.490715: step 5, loss 1.34818, acc 0.62\n",
      "2017-09-11T18:23:10.867995: step 6, loss 1.68529, acc 0.36\n",
      "2017-09-11T18:23:11.239262: step 7, loss 1.59444, acc 0.4\n",
      "2017-09-11T18:23:11.612198: step 8, loss 1.16591, acc 0.48\n",
      "2017-09-11T18:23:11.983602: step 9, loss 1.48803, acc 0.36\n",
      "2017-09-11T18:23:12.355569: step 10, loss 0.891008, acc 0.48\n",
      "2017-09-11T18:23:12.731236: step 11, loss 0.944858, acc 0.48\n",
      "2017-09-11T18:23:13.101285: step 12, loss 1.02992, acc 0.46\n",
      "2017-09-11T18:23:13.474125: step 13, loss 0.85924, acc 0.44\n",
      "2017-09-11T18:23:13.899327: step 14, loss 0.818237, acc 0.56\n",
      "2017-09-11T18:23:14.273639: step 15, loss 0.716853, acc 0.64\n",
      "2017-09-11T18:23:14.648399: step 16, loss 0.812107, acc 0.52\n",
      "2017-09-11T18:23:15.018428: step 17, loss 0.703265, acc 0.52\n",
      "2017-09-11T18:23:15.391330: step 18, loss 0.69204, acc 0.56\n",
      "2017-09-11T18:23:15.766321: step 19, loss 0.712196, acc 0.56\n",
      "2017-09-11T18:23:16.208528: step 20, loss 0.840964, acc 0.42\n",
      "2017-09-11T18:23:16.598378: step 21, loss 0.733934, acc 0.5\n",
      "2017-09-11T18:23:16.971352: step 22, loss 0.848408, acc 0.44\n",
      "2017-09-11T18:23:17.343956: step 23, loss 0.767254, acc 0.5\n",
      "2017-09-11T18:23:17.716870: step 24, loss 0.751366, acc 0.52\n",
      "2017-09-11T18:23:18.208554: step 25, loss 0.694863, acc 0.52\n",
      "2017-09-11T18:23:18.598485: step 26, loss 0.708629, acc 0.56\n",
      "2017-09-11T18:23:18.972749: step 27, loss 0.699476, acc 0.46\n",
      "2017-09-11T18:23:19.376696: step 28, loss 0.713661, acc 0.46\n",
      "2017-09-11T18:23:19.841049: step 29, loss 0.704008, acc 0.52\n",
      "2017-09-11T18:23:20.217542: step 30, loss 0.717566, acc 0.44\n",
      "2017-09-11T18:23:20.590123: step 31, loss 0.706669, acc 0.5\n",
      "2017-09-11T18:23:20.963063: step 32, loss 0.675929, acc 0.6\n",
      "2017-09-11T18:23:21.339095: step 33, loss 0.708172, acc 0.48\n",
      "2017-09-11T18:23:21.803149: step 34, loss 0.683744, acc 0.54\n",
      "2017-09-11T18:23:22.175290: step 35, loss 0.661167, acc 0.52\n",
      "2017-09-11T18:23:22.622450: step 36, loss 0.659736, acc 0.62\n",
      "2017-09-11T18:23:22.996406: step 37, loss 0.689204, acc 0.52\n",
      "2017-09-11T18:23:23.374972: step 38, loss 0.661089, acc 0.58\n",
      "2017-09-11T18:23:23.740240: step 39, loss 0.681429, acc 0.58\n",
      "2017-09-11T18:23:24.102691: step 40, loss 0.64575, acc 0.58\n",
      "2017-09-11T18:23:24.468204: step 41, loss 0.76589, acc 0.4\n",
      "2017-09-11T18:23:24.833954: step 42, loss 0.655047, acc 0.6\n",
      "2017-09-11T18:23:25.205411: step 43, loss 0.596291, acc 0.76\n",
      "2017-09-11T18:23:25.607025: step 44, loss 0.617049, acc 0.74\n",
      "2017-09-11T18:23:26.061450: step 45, loss 0.658832, acc 0.64\n",
      "2017-09-11T18:23:26.443798: step 46, loss 0.689563, acc 0.58\n",
      "2017-09-11T18:23:26.829506: step 47, loss 0.591446, acc 0.64\n",
      "2017-09-11T18:23:27.212726: step 48, loss 0.718175, acc 0.54\n",
      "2017-09-11T18:23:27.637593: step 49, loss 0.595598, acc 0.68\n",
      "2017-09-11T18:23:28.015615: step 50, loss 0.71559, acc 0.54\n",
      "2017-09-11T18:23:28.400418: step 51, loss 0.67701, acc 0.48\n",
      "2017-09-11T18:23:28.821770: step 52, loss 0.694006, acc 0.44\n",
      "2017-09-11T18:23:29.253897: step 53, loss 0.654602, acc 0.58\n",
      "2017-09-11T18:23:29.647346: step 54, loss 0.663243, acc 0.6\n",
      "2017-09-11T18:23:30.069400: step 55, loss 0.64761, acc 0.54\n",
      "2017-09-11T18:23:30.475894: step 56, loss 0.62514, acc 0.64\n",
      "2017-09-11T18:23:30.853575: step 57, loss 0.666873, acc 0.6\n",
      "2017-09-11T18:23:31.272974: step 58, loss 0.663062, acc 0.66\n",
      "2017-09-11T18:23:31.694281: step 59, loss 0.630883, acc 0.66\n",
      "2017-09-11T18:23:32.081861: step 60, loss 0.636519, acc 0.58\n",
      "2017-09-11T18:23:32.461067: step 61, loss 0.663489, acc 0.52\n",
      "2017-09-11T18:23:32.840060: step 62, loss 0.582146, acc 0.68\n",
      "2017-09-11T18:23:33.213198: step 63, loss 0.648009, acc 0.58\n",
      "2017-09-11T18:23:33.675097: step 64, loss 0.636531, acc 0.56\n",
      "2017-09-11T18:23:34.139185: step 65, loss 0.622072, acc 0.62\n",
      "2017-09-11T18:23:34.592818: step 66, loss 0.610003, acc 0.72\n",
      "2017-09-11T18:23:35.031543: step 67, loss 0.655787, acc 0.58\n",
      "2017-09-11T18:23:35.512841: step 68, loss 0.673636, acc 0.54\n",
      "2017-09-11T18:23:35.933797: step 69, loss 0.619464, acc 0.64\n",
      "2017-09-11T18:23:36.384931: step 70, loss 0.632291, acc 0.66\n",
      "2017-09-11T18:23:36.842748: step 71, loss 0.671671, acc 0.56\n",
      "2017-09-11T18:23:37.281809: step 72, loss 0.691874, acc 0.46\n",
      "2017-09-11T18:23:37.680235: step 73, loss 0.695163, acc 0.52\n",
      "2017-09-11T18:23:38.056336: step 74, loss 0.672359, acc 0.64\n",
      "2017-09-11T18:23:38.430334: step 75, loss 0.629817, acc 0.68\n",
      "2017-09-11T18:23:38.800058: step 76, loss 0.598727, acc 0.7\n",
      "2017-09-11T18:23:39.178655: step 77, loss 0.620561, acc 0.54\n",
      "2017-09-11T18:23:39.555058: step 78, loss 0.600092, acc 0.64\n",
      "2017-09-11T18:23:39.929916: step 79, loss 0.865539, acc 0.32\n",
      "2017-09-11T18:23:40.315003: step 80, loss 0.651478, acc 0.56\n",
      "2017-09-11T18:23:40.687260: step 81, loss 0.628912, acc 0.68\n",
      "2017-09-11T18:23:41.061733: step 82, loss 0.612202, acc 0.62\n",
      "2017-09-11T18:23:41.443009: step 83, loss 0.61816, acc 0.64\n",
      "2017-09-11T18:23:41.862646: step 84, loss 0.586895, acc 0.74\n",
      "2017-09-11T18:23:42.285019: step 85, loss 0.653257, acc 0.58\n",
      "2017-09-11T18:23:42.710700: step 86, loss 0.590504, acc 0.7\n",
      "2017-09-11T18:23:43.138263: step 87, loss 0.611118, acc 0.68\n",
      "2017-09-11T18:23:43.605834: step 88, loss 0.573332, acc 0.7\n",
      "2017-09-11T18:23:44.023879: step 89, loss 0.548614, acc 0.74\n",
      "2017-09-11T18:23:44.445011: step 90, loss 0.644807, acc 0.64\n",
      "2017-09-11T18:23:44.867031: step 91, loss 0.658859, acc 0.54\n",
      "2017-09-11T18:23:45.282695: step 92, loss 0.634636, acc 0.66\n",
      "2017-09-11T18:23:45.672624: step 93, loss 0.601466, acc 0.68\n",
      "2017-09-11T18:23:46.108321: step 94, loss 0.687833, acc 0.54\n",
      "2017-09-11T18:23:46.510505: step 95, loss 0.637089, acc 0.6\n",
      "2017-09-11T18:23:46.889150: step 96, loss 0.549892, acc 0.72\n",
      "2017-09-11T18:23:47.264746: step 97, loss 0.584655, acc 0.66\n",
      "2017-09-11T18:23:47.638531: step 98, loss 0.588555, acc 0.68\n",
      "2017-09-11T18:23:48.018315: step 99, loss 0.540972, acc 0.72\n",
      "2017-09-11T18:23:48.446588: step 100, loss 0.676275, acc 0.58\n",
      "Saved model checkpoint to /home/aayush/robust-large-margin-cnn-develop/runs/1505134384/checkpoints/model-100\n",
      "\n",
      "2017-09-11T18:23:50.782874: step 101, loss 0.579397, acc 0.64\n",
      "2017-09-11T18:23:51.169832: step 102, loss 0.542103, acc 0.8\n",
      "2017-09-11T18:23:51.543357: step 103, loss 0.496351, acc 0.84\n",
      "2017-09-11T18:23:51.921044: step 104, loss 0.589655, acc 0.72\n",
      "2017-09-11T18:23:52.297565: step 105, loss 0.575065, acc 0.7\n",
      "2017-09-11T18:23:52.673714: step 106, loss 0.456263, acc 0.84\n",
      "2017-09-11T18:23:53.053990: step 107, loss 0.530344, acc 0.68\n",
      "2017-09-11T18:23:53.429879: step 108, loss 0.525877, acc 0.72\n",
      "2017-09-11T18:23:53.855395: step 109, loss 0.514766, acc 0.74\n",
      "2017-09-11T18:23:54.279775: step 110, loss 0.492155, acc 0.72\n",
      "2017-09-11T18:23:54.702878: step 111, loss 0.547625, acc 0.64\n",
      "2017-09-11T18:23:55.124469: step 112, loss 0.497049, acc 0.74\n",
      "2017-09-11T18:23:55.548705: step 113, loss 0.495022, acc 0.76\n",
      "2017-09-11T18:23:55.970218: step 114, loss 0.487514, acc 0.82\n",
      "2017-09-11T18:23:56.393256: step 115, loss 0.575171, acc 0.66\n",
      "2017-09-11T18:23:56.820018: step 116, loss 0.614485, acc 0.64\n",
      "2017-09-11T18:23:57.266337: step 117, loss 0.65494, acc 0.6\n",
      "2017-09-11T18:23:57.830614: step 118, loss 0.498742, acc 0.74\n",
      "2017-09-11T18:23:58.368390: step 119, loss 0.486304, acc 0.72\n",
      "2017-09-11T18:23:58.911205: step 120, loss 0.522, acc 0.7\n",
      "2017-09-11T18:23:59.432367: step 121, loss 0.518243, acc 0.62\n",
      "2017-09-11T18:24:00.025545: step 122, loss 0.564371, acc 0.7\n",
      "2017-09-11T18:24:00.567126: step 123, loss 0.380598, acc 0.88\n",
      "2017-09-11T18:24:01.098763: step 124, loss 0.426751, acc 0.78\n",
      "2017-09-11T18:24:01.613415: step 125, loss 0.430195, acc 0.84\n",
      "2017-09-11T18:24:02.096663: step 126, loss 0.404552, acc 0.82\n",
      "2017-09-11T18:24:02.613050: step 127, loss 0.496408, acc 0.76\n",
      "2017-09-11T18:24:03.146431: step 128, loss 0.484986, acc 0.72\n",
      "2017-09-11T18:24:03.680978: step 129, loss 0.495513, acc 0.8\n",
      "2017-09-11T18:24:04.165568: step 130, loss 0.350288, acc 0.84\n",
      "2017-09-11T18:24:04.650757: step 131, loss 0.445222, acc 0.76\n",
      "2017-09-11T18:24:05.154255: step 132, loss 0.554429, acc 0.74\n",
      "2017-09-11T18:24:05.633848: step 133, loss 0.395401, acc 0.8\n",
      "2017-09-11T18:24:06.095529: step 134, loss 0.356316, acc 0.88\n",
      "2017-09-11T18:24:06.531530: step 135, loss 0.582149, acc 0.64\n",
      "2017-09-11T18:24:06.956912: step 136, loss 0.464854, acc 0.78\n",
      "2017-09-11T18:24:07.389611: step 137, loss 0.365567, acc 0.84\n",
      "2017-09-11T18:24:07.806121: step 138, loss 0.379516, acc 0.88\n",
      "2017-09-11T18:24:08.231998: step 139, loss 0.38009, acc 0.86\n",
      "2017-09-11T18:24:08.703463: step 140, loss 0.297363, acc 0.86\n",
      "2017-09-11T18:24:09.248089: step 141, loss 0.272375, acc 0.92\n",
      "2017-09-11T18:24:09.655141: step 142, loss 0.306743, acc 0.88\n",
      "2017-09-11T18:24:10.032158: step 143, loss 0.228497, acc 0.94\n",
      "2017-09-11T18:24:10.411218: step 144, loss 0.283931, acc 0.96\n",
      "2017-09-11T18:24:10.794250: step 145, loss 0.312002, acc 0.9\n",
      "2017-09-11T18:24:11.180327: step 146, loss 0.22001, acc 0.94\n",
      "2017-09-11T18:24:11.566830: step 147, loss 0.276589, acc 0.9\n",
      "2017-09-11T18:24:11.949238: step 148, loss 0.322336, acc 0.86\n",
      "2017-09-11T18:24:12.327221: step 149, loss 0.231476, acc 0.94\n",
      "2017-09-11T18:24:12.703648: step 150, loss 0.275242, acc 0.88\n",
      "2017-09-11T18:24:13.080595: step 151, loss 0.214381, acc 0.92\n",
      "2017-09-11T18:24:13.464791: step 152, loss 0.245587, acc 0.94\n",
      "2017-09-11T18:24:13.884147: step 153, loss 0.255815, acc 0.9\n",
      "2017-09-11T18:24:14.309465: step 154, loss 0.299284, acc 0.9\n",
      "2017-09-11T18:24:14.731474: step 155, loss 0.204891, acc 0.98\n",
      "2017-09-11T18:24:15.174071: step 156, loss 0.287479, acc 0.88\n",
      "2017-09-11T18:24:15.611245: step 157, loss 0.21665, acc 0.94\n",
      "2017-09-11T18:24:16.051020: step 158, loss 0.247862, acc 0.92\n",
      "2017-09-11T18:24:16.475079: step 159, loss 0.268581, acc 0.86\n",
      "2017-09-11T18:24:16.898996: step 160, loss 0.294304, acc 0.84\n",
      "2017-09-11T18:24:17.322792: step 161, loss 0.239394, acc 0.86\n",
      "2017-09-11T18:24:17.704445: step 162, loss 0.171264, acc 0.98\n",
      "2017-09-11T18:24:18.079160: step 163, loss 0.154832, acc 0.96\n",
      "2017-09-11T18:24:18.452411: step 164, loss 0.213331, acc 0.9\n",
      "2017-09-11T18:24:18.826329: step 165, loss 0.174556, acc 0.94\n",
      "2017-09-11T18:24:19.202263: step 166, loss 0.216538, acc 0.94\n",
      "2017-09-11T18:24:19.583397: step 167, loss 0.170556, acc 0.98\n",
      "2017-09-11T18:24:19.961041: step 168, loss 0.163974, acc 0.96\n",
      "2017-09-11T18:24:20.348160: step 169, loss 0.170155, acc 0.96\n",
      "2017-09-11T18:24:20.736871: step 170, loss 0.189466, acc 0.9\n",
      "2017-09-11T18:24:21.114762: step 171, loss 0.206803, acc 0.88\n",
      "2017-09-11T18:24:21.507651: step 172, loss 0.179528, acc 0.94\n",
      "2017-09-11T18:24:21.943301: step 173, loss 0.254352, acc 0.92\n",
      "2017-09-11T18:24:22.374810: step 174, loss 0.166781, acc 0.96\n",
      "2017-09-11T18:24:22.797837: step 175, loss 0.113366, acc 0.98\n",
      "2017-09-11T18:24:23.235097: step 176, loss 0.110096, acc 0.98\n",
      "2017-09-11T18:24:23.673149: step 177, loss 0.200234, acc 0.94\n",
      "2017-09-11T18:24:24.095201: step 178, loss 0.127084, acc 0.94\n",
      "2017-09-11T18:24:24.523310: step 179, loss 0.187548, acc 0.94\n",
      "2017-09-11T18:24:24.962961: step 180, loss 0.127251, acc 0.96\n",
      "2017-09-11T18:24:25.392168: step 181, loss 0.0884234, acc 1\n",
      "2017-09-11T18:24:25.858047: step 182, loss 0.127291, acc 0.98\n",
      "2017-09-11T18:24:26.234743: step 183, loss 0.105146, acc 0.98\n",
      "2017-09-11T18:24:26.610615: step 184, loss 0.0782739, acc 0.98\n",
      "2017-09-11T18:24:26.986844: step 185, loss 0.114772, acc 0.98\n",
      "2017-09-11T18:24:27.362248: step 186, loss 0.0546172, acc 1\n",
      "2017-09-11T18:24:27.736851: step 187, loss 0.091311, acc 1\n",
      "2017-09-11T18:24:28.115692: step 188, loss 0.104979, acc 0.98\n",
      "2017-09-11T18:24:28.490985: step 189, loss 0.120068, acc 0.96\n",
      "2017-09-11T18:24:28.867609: step 190, loss 0.0993883, acc 0.98\n",
      "2017-09-11T18:24:29.252076: step 191, loss 0.0919667, acc 0.98\n",
      "2017-09-11T18:24:29.659785: step 192, loss 0.0723649, acc 1\n",
      "2017-09-11T18:24:30.080201: step 193, loss 0.0770988, acc 1\n",
      "2017-09-11T18:24:30.504962: step 194, loss 0.061121, acc 0.98\n",
      "2017-09-11T18:24:30.936391: step 195, loss 0.0779372, acc 0.98\n",
      "2017-09-11T18:24:31.430852: step 196, loss 0.0969756, acc 0.98\n",
      "2017-09-11T18:24:31.857143: step 197, loss 0.117359, acc 0.94\n",
      "2017-09-11T18:24:32.278438: step 198, loss 0.135851, acc 0.96\n",
      "2017-09-11T18:24:32.701828: step 199, loss 0.0869528, acc 0.96\n",
      "2017-09-11T18:24:33.126390: step 200, loss 0.0505072, acc 1\n",
      "Saved model checkpoint to /home/aayush/robust-large-margin-cnn-develop/runs/1505134384/checkpoints/model-200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training ...\")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=sequence_length,\n",
    "            num_classes=num_classes,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=filter_sizes,\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Checkpoints and logs will be written into {}\\n\".format(out_dir))\n",
    "\n",
    "        # Creating heckpoint directory\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embedding})\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "        \n",
    "        batches = utils.batch_iter(\n",
    "        list(zip(X_train[:1000], Y_train[:1000])), batch_size, num_epochs)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
