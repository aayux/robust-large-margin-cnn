{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_utils as utils\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "#from text_cnn import TextCNN\n",
    "from data_utils import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Dataset loaded. Preparing data and loading embeddings ...\n",
      "Embeddings loaded. Initialising model hyperparameters ...\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 128\n",
    "num_classes = 2\n",
    "vocab_size = 75099\n",
    "embedding_dim = 300\n",
    "\n",
    "print (\"Loading Dataset ...\")\n",
    "dataset = IMDBDataset('/home/aayush/robust-large-margin-cnn-develop/data/aclImdb/train', '/home/aayush/robust-large-margin-cnn-develop/data/vocab.pckl')\n",
    "X, Y = dataset.load()\n",
    "print (\"Dataset loaded. Preparing data and loading embeddings ...\")\n",
    "\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "\n",
    "X_train = X[shuffle_indices]\n",
    "Y_train = Y[shuffle_indices]\n",
    "\n",
    "embedding_path = '/home/aayush/robust-large-margin-cnn-develop/data/embeddings.npy'\n",
    "embedding = utils.load_embeddings(embedding_path, vocab_size, embedding_dim)\n",
    "print (\"Embeddings loaded. Initialising model hyperparameters ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75099, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, \n",
    "      l2_reg_lambda=0.0, jac_reg=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "       \n",
    "        # Embedding layer\n",
    "        self.word_embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "        self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "        self.embedding_init = self.word_embedding.assign(self.embedding_placeholder)\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.word_embedding, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        print('Embedding: {}'.format(self.embedded_chars_expanded.get_shape()))\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        layer_outputs = []\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv%s-maxpool-1\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                with tf.variable_scope((\"conv%s-maxpool-1\" % filter_size),  reuse=None):\n",
    "                    W = self.init_weight(filter_shape)\n",
    "                b = self.init_bias([num_filters])\n",
    "                conv = self.convolution(self.embedded_chars_expanded, W)\n",
    "                print('Conv1-{}: {}'.format(filter_size, conv.get_shape()))\n",
    "                \n",
    "                # Apply nonlinearity\n",
    "                h = self.non_linearity(conv, b)\n",
    "                \n",
    "                # Maxpooling over the outputs\n",
    "                ksize = [1, sequence_length // 2 - filter_size, 1, 1]\n",
    "                pooled = self.maxpool(h, ksize)\n",
    "                print('Maxpool1-{}: {}'.format(filter_size, pooled.get_shape()))\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        layer_outputs = pooled_outputs\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        print('Concatenated: {}'.format(self.h_pool.get_shape()))\n",
    "        \n",
    "        # Second convolution + maxpool layer\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            filter_shape = [4, 1, self.h_pool.get_shape()[3].value, num_filters // 2]\n",
    "            with tf.variable_scope(\"conv-maxpool-2\",  reuse=None):\n",
    "                W = self.init_weight(filter_shape)\n",
    "            b = self.init_bias([num_filters // 2])\n",
    "            conv = self.convolution(self.h_pool, W)\n",
    "            print('Conv2: {}'.format(conv.get_shape()))\n",
    "            \n",
    "            h = self.non_linearity(conv, b)\n",
    "            \n",
    "            ksize = [1, 128 // 8 - 1, 1, 1]\n",
    "            self.pooled_2 = self.maxpool(h, ksize)\n",
    "\n",
    "            layer_outputs.append(self.pooled_2)\n",
    "            print('Maxpool2: {}'.format(self.pooled_2.get_shape()))\n",
    "        \n",
    "        # Third convolution + maxpool layer            \n",
    "        with tf.name_scope(\"conv-maxpool-3\"):\n",
    "            filter_shape = [6, 1, self.pooled_2.get_shape()[3].value, num_filters // 2]\n",
    "            with tf.variable_scope(\"conv-maxpool-3\",  reuse=None):\n",
    "                W = self.init_weight(filter_shape)\n",
    "            b = self.init_bias([num_filters // 2])\n",
    "            conv = self.convolution(self.pooled_2, W)\n",
    "            print('Conv3: {}'.format(conv.get_shape()))\n",
    "            \n",
    "            h = self.non_linearity(conv, b)\n",
    "            \n",
    "            ksize=[1, conv.get_shape()[1].value, 1, 1]\n",
    "            self.pooled_3 = self.maxpool(h, ksize)\n",
    "            \n",
    "            layer_outputs.append(self.pooled_3)\n",
    "            print('Maxpool3: {}'.format(self.pooled_3.get_shape()))\n",
    "        \n",
    "        # Flatten into a long feature vector\n",
    "        self.h_pool_flat = tf.reshape(self.pooled_3, [-1, num_filters // 2])\n",
    "        print('Flattened: {}'.format(self.h_pool_flat.get_shape()))\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = self.add_dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            with tf.variable_scope(\"output\",  reuse=None):\n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[num_filters // 2, num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            \n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            softmax_pred = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(softmax_pred) + l2_reg_lambda * l2_loss\n",
    "        \n",
    "        # Jacobian Regularizer            \n",
    "        with tf.name_scope(\"jacobian_reg\"):\n",
    "            if jac_reg > 0.0:\n",
    "                layer_names = [\"conv4-maxpool-1\", \"conv5-maxpool-1\", \"conv-maxpool-2\", \"conv-maxpool-3\"]\n",
    "                for idx, scope in enumerate(layer_names):\n",
    "                    with tf.variable_scope(scope, reuse=True):\n",
    "                        W = tf.get_variable(\"W\")\n",
    "                        # jacobian matrix of network output w.r.t. the outputs of layer L\n",
    "                        # dimension: (batch_size, width, height, number of filters)\n",
    "                        \n",
    "                        #g_x = tf.gradients(tf.reduce_sum(tf.multiply(self.input_y, self.scores)), layer_outputs[idx])\n",
    "                        g_x = tf.gradients(tf.multiply(self.input_y, self.scores), layer_outputs[idx])\n",
    "                        \n",
    "                        # reshape (batch_size, height=1, width, number of filters) to (batch_size*width, number of filters)\n",
    "                        g_x = tf.reshape(g_x, shape=[-1, tf.shape(W)[3]])\n",
    "                                                \n",
    "                        # covariance matrix of jacobian vectors\n",
    "                        reg = tf.matmul(tf.transpose(g_x), g_x)\n",
    "                                                \n",
    "                        # parameter update\n",
    "                        W -= 1e-3 * jac_reg * tf.tensordot(reg, W, axes=[[1], [0]])\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "    def init_weight(self, shape):\n",
    "        return tf.get_variable(\"W\", shape, dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "    def init_bias(self, shape):\n",
    "        return tf.Variable(tf.constant(0.1, shape=shape), name=\"b\")\n",
    "\n",
    "    def non_linearity(self, conv, bias):\n",
    "        return tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n",
    "\n",
    "    def add_dropout(self, drop_input, keep_prob):\n",
    "        return tf.nn.dropout(drop_input, keep_prob)\n",
    "\n",
    "    def convolution(self, conv_input, weights):\n",
    "        conv = tf.nn.conv2d(\n",
    "            conv_input,\n",
    "            weights,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"convolution\")\n",
    "        return conv\n",
    "\n",
    "    def maxpool(self, pool_input, ksize):\n",
    "        pooled = tf.nn.max_pool(\n",
    "                    pool_input,\n",
    "                    ksize=ksize,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "        return pooled\n",
    "\n",
    "    def fully_conected(self, fc_in, in_shape, out_shape):\n",
    "        W =  tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b = self.init_bias(out_shape)\n",
    "        return tf.nn.xw_plus_b(fc_in, W, b, name=\"fully_connected\")\n",
    "    \n",
    "    def jacobian():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##To do\n",
    "- Add Fully Conected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "filter_sizes = [4, 5]\n",
    "num_filters = 64\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "jac_reg = 0.1\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 50\n",
    "num_epochs = 10\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Embedding: (?, 128, 300, 1)\n",
      "Conv1-4: (?, 125, 1, 64)\n",
      "Maxpool1-4: (?, 66, 1, 64)\n",
      "Conv1-5: (?, 124, 1, 64)\n",
      "Maxpool1-5: (?, 66, 1, 64)\n",
      "Concatenated: (?, 66, 1, 128)\n",
      "Conv2: (?, 63, 1, 32)\n",
      "Maxpool2: (?, 49, 1, 32)\n",
      "Conv3: (?, 44, 1, 32)\n",
      "Maxpool3: (?, 1, 1, 32)\n",
      "Flattened: (?, 32)\n",
      "INFO:tensorflow:Summary name conv4-maxpool-1/W:0/grad/hist is illegal; using conv4-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/W:0/grad/hist is illegal; using conv4-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/W:0/grad/sparsity is illegal; using conv4-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/W:0/grad/sparsity is illegal; using conv4-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/b:0/grad/hist is illegal; using conv4-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/b:0/grad/hist is illegal; using conv4-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/b:0/grad/sparsity is illegal; using conv4-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv4-maxpool-1/b:0/grad/sparsity is illegal; using conv4-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/W:0/grad/hist is illegal; using conv5-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/W:0/grad/hist is illegal; using conv5-maxpool-1/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/W:0/grad/sparsity is illegal; using conv5-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/W:0/grad/sparsity is illegal; using conv5-maxpool-1/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/b:0/grad/hist is illegal; using conv5-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/b:0/grad/hist is illegal; using conv5-maxpool-1/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/b:0/grad/sparsity is illegal; using conv5-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv5-maxpool-1/b:0/grad/sparsity is illegal; using conv5-maxpool-1/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/aayush/robust-large-margin-cnn-develop/runs/1508254256\n",
      "\n",
      "Generating batch iterator ...\n",
      "2017-10-17T21:00:58.416222: step 1, loss 3.0666, acc 0.58\n",
      "2017-10-17T21:00:58.835691: step 2, loss 1.57365, acc 0.56\n",
      "2017-10-17T21:00:59.221162: step 3, loss 0.90876, acc 0.54\n",
      "2017-10-17T21:00:59.617773: step 4, loss 0.640487, acc 0.64\n",
      "2017-10-17T21:01:00.000913: step 5, loss 0.741047, acc 0.64\n",
      "2017-10-17T21:01:00.396367: step 6, loss 0.705256, acc 0.58\n",
      "2017-10-17T21:01:00.782994: step 7, loss 0.769182, acc 0.58\n",
      "2017-10-17T21:01:01.166538: step 8, loss 0.880539, acc 0.38\n",
      "2017-10-17T21:01:01.558171: step 9, loss 0.699748, acc 0.62\n",
      "2017-10-17T21:01:01.981362: step 10, loss 0.706003, acc 0.5\n",
      "2017-10-17T21:01:02.379863: step 11, loss 0.769595, acc 0.52\n",
      "2017-10-17T21:01:02.766435: step 12, loss 0.715894, acc 0.58\n",
      "2017-10-17T21:01:03.157492: step 13, loss 0.657667, acc 0.56\n",
      "2017-10-17T21:01:03.597730: step 14, loss 0.768155, acc 0.42\n",
      "2017-10-17T21:01:04.003660: step 15, loss 0.711104, acc 0.48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fdb0ddce7148>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcheckpoint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fdb0ddce7148>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     68\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m     69\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aayush/Documents/anaconda3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training ...\")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=sequence_length,\n",
    "            num_classes=num_classes,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=filter_sizes,\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            jac_reg=jac_reg)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)             \n",
    "                \n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embedding})\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = utils.batch_iter(\n",
    "        list(zip(X_train[:1000], Y_train[:1000])), batch_size, num_epochs)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
